{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UJlsScHiVagq7j8TPXfYmFgHEGOwXbsd","timestamp":1741386980307},{"file_id":"1vo4YpGTihn6BZ0QqQGgGgdXlENjZxpnT","timestamp":1741278727695}],"gpuType":"T4","authorship_tag":"ABX9TyMsYhY0zveeOJ7BHXLzsccS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c8279d3df23240b69387b5d4e9b0d781":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_253c2bb18f9f49bf8ee5cdef9816e20c","IPY_MODEL_2284145f93d340479087fb0044b6181d","IPY_MODEL_598f5fa7960846ac91f4117c261e9ce3"],"layout":"IPY_MODEL_b058296187434d7db6431b8b09241437"}},"253c2bb18f9f49bf8ee5cdef9816e20c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07a2f46c78274d07929a65519b72ee44","placeholder":"​","style":"IPY_MODEL_ae725c5529d54fb2b2b41e80ee09e2a9","value":"Filter: 100%"}},"2284145f93d340479087fb0044b6181d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5fa669c4b4e40dba2a02350a561fa07","max":2683,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62660b7ad096494f990ce10dabdc1fca","value":2683}},"598f5fa7960846ac91f4117c261e9ce3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_762c0d0fb1df47f2a5a00563bfe03fdf","placeholder":"​","style":"IPY_MODEL_d5d8bd60e40240a19e111aa734b22ecb","value":" 2683/2683 [00:09&lt;00:00, 296.59 examples/s]"}},"b058296187434d7db6431b8b09241437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07a2f46c78274d07929a65519b72ee44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae725c5529d54fb2b2b41e80ee09e2a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fa669c4b4e40dba2a02350a561fa07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62660b7ad096494f990ce10dabdc1fca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"762c0d0fb1df47f2a5a00563bfe03fdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5d8bd60e40240a19e111aa734b22ecb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a7c9f877944e15b70756091589788e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_125463dfa5b84794ab1be57a99aaf29b","IPY_MODEL_878f8fe1c9b74f5c9a3ed0d7cb79fcd5","IPY_MODEL_3697104b569944acac3ccfd785a270a2"],"layout":"IPY_MODEL_944acf19c2044da189303097b88c546e"}},"125463dfa5b84794ab1be57a99aaf29b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_623cead1203e4cf8aaacf4556280dd25","placeholder":"​","style":"IPY_MODEL_533ab7c666d64910a62c858557b8b0b2","value":"Filter: 100%"}},"878f8fe1c9b74f5c9a3ed0d7cb79fcd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e356d91d85a142b7b4726108746250f6","max":337,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50903b60b9b94334903099b3af031072","value":337}},"3697104b569944acac3ccfd785a270a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f8b55afbaaa4167b0900d3d5f4da7ca","placeholder":"​","style":"IPY_MODEL_73b539d7aced4ddca1bd581e3823f6a3","value":" 337/337 [00:01&lt;00:00, 302.30 examples/s]"}},"944acf19c2044da189303097b88c546e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623cead1203e4cf8aaacf4556280dd25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"533ab7c666d64910a62c858557b8b0b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e356d91d85a142b7b4726108746250f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50903b60b9b94334903099b3af031072":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f8b55afbaaa4167b0900d3d5f4da7ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b539d7aced4ddca1bd581e3823f6a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a621399936149588b9e64755438b400":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03ada529c952459d928a78f6f82f6420","IPY_MODEL_fdaa5f6fb7b94180be5055cae8f7b1cc","IPY_MODEL_373e4a2e09de44f985b1ca22a9df99be"],"layout":"IPY_MODEL_0c213786dd1e48fb95994f6e15f60b95"}},"03ada529c952459d928a78f6f82f6420":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32a6166af9f54c3c8246844024c75d94","placeholder":"​","style":"IPY_MODEL_c5b02240e2fa46da847d962c2248b3b3","value":"Filter: 100%"}},"fdaa5f6fb7b94180be5055cae8f7b1cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18446fd583514bf7a28bff26061d7e72","max":343,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48926d629ac4489aab7ddc1513bfecc5","value":343}},"373e4a2e09de44f985b1ca22a9df99be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bae0058a97c44b1089a5cec80afc32c5","placeholder":"​","style":"IPY_MODEL_5003d6934f3a41f58faded828ea74840","value":" 343/343 [00:01&lt;00:00, 315.17 examples/s]"}},"0c213786dd1e48fb95994f6e15f60b95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32a6166af9f54c3c8246844024c75d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b02240e2fa46da847d962c2248b3b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18446fd583514bf7a28bff26061d7e72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48926d629ac4489aab7ddc1513bfecc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bae0058a97c44b1089a5cec80afc32c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5003d6934f3a41f58faded828ea74840":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Requirements\n","If you are running this tutorial on collab we strongly suggest switching to a GPU environment - especially for the deep learning stage.\n","\n","If you are running it locally and have no access to a dedicated and/or compatible GPU, or are out of resources on collab, worry not! Simply run the experiments for less epochs so that you can follow along!\n"],"metadata":{"id":"Mf2OQc7YTwCt"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0KTi-atV_84","executionInfo":{"status":"ok","timestamp":1741387005055,"user_tz":0,"elapsed":6952,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"83a75f84-92c7-4735-a7bc-5d5127ca33f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Collecting PyWavelets\n","  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting speechpy\n","  Downloading speechpy-2.4-py2.py3-none-any.whl.metadata (407 bytes)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechpy) (1.13.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading speechpy-2.4-py2.py3-none-any.whl (9.5 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, PyWavelets, dill, speechpy, multiprocess, datasets\n","Successfully installed PyWavelets-1.8.0 datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 speechpy-2.4 xxhash-3.5.0\n"]}],"source":["!pip install datasets PyWavelets speechpy"]},{"cell_type":"code","source":["import os\n","\n","import librosa\n","import logging\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pywt\n","import scipy.io as sio\n","import scipy.signal\n","import random\n","import re\n","\n","from scipy.io import wavfile\n","\n","from PIL import Image\n","from tqdm import tqdm\n"],"metadata":{"id":"2Q7_fcgfWC3M","executionInfo":{"status":"ok","timestamp":1741387006203,"user_tz":0,"elapsed":1145,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Part 1. Signal Processing\n","\n","You have access to processed samples from the [2022 CirCor DigiScope dataset](https://physionet.org/content/circor-heart-sound/1.0.3/) available on the [HuggingFace Dataset Hub](https://huggingface.co/docs/hub/datasets-overview).\n","\n","Please click [this link](https://huggingface.co/datasets/miguellmartins/circor-digiscope-physionet22-processed) and inspect the attributes of the original (raw) dataset. This HF Dataset has two splits: \"original\" and \"processed\".\n","\n","The \"processed\" version contains the resulting sounds after filtering and denoising (using the techniques we will discuss in this section).\n","\n","These data may be interfaced using the [HuggingFace API](https://huggingface.co/docs/). The actual files are stored locally and remotely using [Apache Parquet](https://parquet.apache.org/).\n"],"metadata":{"id":"hEVwsR3yUdUQ"}},{"cell_type":"code","source":["from datasets import load_dataset, Audio, DatasetDict"],"metadata":{"id":"FEFGwZjrW7U0","executionInfo":{"status":"ok","timestamp":1741387007718,"user_tz":0,"elapsed":1507,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["circor = load_dataset('miguellmartins/circor-digiscope-physionet22-processed')"],"metadata":{"id":"qo6B2NqTWC78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspect the attributes of the datasets. Store splits in sepearate objects"],"metadata":{"id":"WQC6tpRGXKZs"}},{"cell_type":"code","source":["print(circor)\n","original = circor['original']\n","processed = circor['processed']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lHF_cl1XOHa","executionInfo":{"status":"ok","timestamp":1741387035465,"user_tz":0,"elapsed":38,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"e49e1339-ef87-43cf-fb54-9ff0e3dd5825"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    original: Dataset({\n","        features: ['filename', 'recording', 'recording_label', 'heart_state_labels'],\n","        num_rows: 3363\n","    })\n","    processed: Dataset({\n","        features: ['filename', 'recording', 'recording_label', 'heart_state_labels'],\n","        num_rows: 3363\n","    })\n","})\n"]}]},{"cell_type":"code","source":["# You can check the name of the original wav file, the waveform,\n","# and the sampling rate of each recording\n","original[0]['recording']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHe6UOLQUGFt","executionInfo":{"status":"ok","timestamp":1741387064587,"user_tz":0,"elapsed":29123,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"3c84833e-c63a-47d1-8a13-24a7abbaabb3"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'path': '13918_AV.wav',\n"," 'array': array([-0.0100708 , -0.00579834, -0.00692749, ..., -0.00238037,\n","         0.00396729,  0.00717163]),\n"," 'sampling_rate': 4000}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## 1.1 Pick a random sound and use the below function to visualize the sound and its annotation\n","\n","Remember that each heart state is codified with categorical labels. So\n","* 1: S1\n","* 2: Systole\n","* 3: S2\n","* 4: Diastole\n"],"metadata":{"id":"s3O481NWVw84"}},{"cell_type":"code","source":["def plot_sound_and_label(x, y, sampling_rate=4000):\n","    assert len(x) == len(y)\n","    number_of_samples = len(x)\n","    # Time (duration) = T_i / sample_rate\n","    time = np.arange(number_of_samples) / sampling_rate\n","\n","    # Plotting x and y together\n","    fig, ax1 = plt.subplots(figsize=(12, 6))\n","\n","    # Plot x on the primary y-axis\n","    ax1.plot(time, x, label='x (PCG)', color='b')\n","    ax1.set_xlabel('Time (s)')\n","    ax1.set_ylabel('Amplitude of o', color='b')\n","    ax1.tick_params(axis='y', labelcolor='b')\n","\n","    # Create a secondary y-axis for y\n","    ax2 = ax1.twinx()\n","    ax2.step(time, y, label='y (Heart Sattes)', color='r', where='post', linewidth=2)\n","    ax2.set_ylabel('y (Labels)', color='r')\n","    ax2.set_yticks([1, 2, 3, 4])\n","    ax2.tick_params(axis='y', labelcolor='r')\n","\n","    fig.suptitle('PCG Amplitude and Heart Sound Labels')\n","    ax1.legend(loc='upper left')\n","    ax2.legend(loc='upper right')\n","    plt.show()"],"metadata":{"id":"0ZNLSog6V5U0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_idx =   # chose an idx i and visualize it\n","\n","\n","plot_sound_and_label(x=original[sample_idx]['recording']['array'],\n","                     y=original[sample_idx]['heart_state_labels'],\n","                     sampling_rate=original[sample_idx]['recording']['sampling_rate'])"],"metadata":{"id":"MBVeCvL-TnDb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.1.1 Band-pass filtering\n","The information about the heart typically is assumed to be in [the [25-400] Hz band](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242001).\n","\n","We are specifically using [Butterworth filters](https://en.wikipedia.org/wiki/Butterworth_filter) for this purpose.\n","\n","Run the cells below to visualize these filters.\n","\n","Apply the band-pass filters to a sound of your choice from the dataset."],"metadata":{"id":"q6MLV86yU0Kl"}},{"cell_type":"code","source":["def plot_filter_responses(low_pass_fs, high_pass_fs, sampling_rate=4000, filter_order=2):\n","    \"\"\"\n","    Plots the frequency responses of a highpass and a lowpass Butterworth filter.\n","\n","    Parameters:\n","    - low_pass_fs: float, the cutoff frequency for the highpass filter (Hz)\n","    - high_pass_fs: float, the cutoff frequency for the lowpass filter (Hz)\n","    - sampling_rate: float, the sampling rate of the signals (Hz)\n","    - filter_order: int, the order of the Butterworth filter (default is 2)\n","    \"\"\"\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","    from scipy.signal import butter, sosfreqz\n","\n","    # Design the highpass filter\n","    sos_hp = butter(N=filter_order, Wn=low_pass_fs, btype='highpass', analog=False, fs=sampling_rate, output='sos')\n","    # Design the lowpass filter\n","    sos_lp = butter(N=filter_order, Wn=high_pass_fs, btype='lowpass', analog=False, fs=sampling_rate, output='sos')\n","\n","    # Frequency response for the highpass filter\n","    w_hp, h_hp = sosfreqz(sos_hp, fs=sampling_rate)\n","    # Frequency response for the lowpass filter\n","    w_lp, h_lp = sosfreqz(sos_lp, fs=sampling_rate)\n","\n","    # Plot the frequency response of both filters\n","    plt.figure(figsize=(12, 6))\n","\n","    # Plot for the highpass filter\n","    plt.subplot(2, 1, 1)\n","    plt.plot(w_hp, 20 * np.log10(np.abs(h_hp)), label='Highpass Filter')\n","    plt.title('Highpass Filter Frequency Response')\n","    plt.xlabel('Frequency (Hz)')\n","    plt.ylabel('Amplitude (dB)')\n","    plt.grid()\n","    plt.axvline(low_pass_fs, color='red', linestyle='--', label=f'Cutoff: {low_pass_fs} Hz')\n","    plt.legend()\n","\n","    # Plot for the lowpass filter\n","    plt.subplot(2, 1, 2)\n","    plt.plot(w_lp, 20 * np.log10(np.abs(h_lp)), label='Lowpass Filter')\n","    plt.title('Lowpass Filter Frequency Response')\n","    plt.xlabel('Frequency (Hz)')\n","    plt.ylabel('Amplitude (dB)')\n","    plt.grid()\n","    plt.axvline(high_pass_fs, color='red', linestyle='--', label=f'Cutoff: {high_pass_fs} Hz')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_filter_responses(low_pass_fs=..., high_pass_fs=...)"],"metadata":{"id":"-3G8-04qV19P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_idx = # choose a sample idx\n","sound = original[sample_idx]['recording']['array']\n","sampling_rate = original[sample_idx]['recording']['sampling_rate']\n","sos_hp = scipy.signal.butter(N=2, Wn=45, btype='highpass', analog=False, fs=sampling_rate,\n","                                     output='sos')\n","sos_lp = scipy.signal.butter(N=2, Wn=400, btype='lowpass', analog=False, fs=sampling_rate,\n","                              output='sos')\n","filtered = scipy.signal.sosfilt(sos_hp, sound)\n","filtered = scipy.signal.sosfilt(sos_lp, filtered)"],"metadata":{"id":"I1qlxo0cZuaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1.2 Denoising -  Averaging Theory\n","The section of denoising in this tutorial follows [Messer et al.](https://www.sciencedirect.com/science/article/pii/S0026269201000957).\n","\n","\n","Suppose a Source $\\mathbf{S}$ is corrupted additively by i.i.d. Gaussian noise $\\epsilon_i$. We observe $\\mathbf{X}$ and not $\\mathbf{S}$ in our measurements such that:\n","\n","$$X_i = S + \\epsilon_i, i=1,\\ldots, N$$\n","\n","If one computes its variance:\n","\n","$$\\text{Var}\\left(\\mathbf{S} + \\frac{1}{N} \\sum_{j=1}^{N}  \\epsilon_j\\right) = \\text{Var}(\\mathbf{S}) + \\text{Var}\\left(\\frac{1}{N} \\sum_{j=1}^{N}  \\epsilon_j \\right) = \\text{Var}(\\mathbf{S}) + \\frac{T_i\\sigma^2}{N^2}= \\text{Var}(\\mathbf{S}) + \\frac{\\sigma^2}{N}$$\n","\n","one observes that the **standard deviation** of the random terms will shrink as $\\frac{\\sigma}{\\sqrt(N)}$.\n","\n","Due to the periodic nature of heart sounds and their stationarity (at least in a \"short\" period of time), we can think of $\\mathbf{S}$ to be the expected waveform of the heart cycle in a recording, i.e. the *characteristic heart cycle* of a patient.\n","\n","## 2.1.3 Wavevet Denoising\n","\n","In practice, the analysis of a characteristic heartbeat for most downstream\n","applications is not very useful. Even under (quasi)-stationary assumptions, there may be other phenomena present in the signal such as murmurs that may occur in all states of the heart cycles and across several frequency bands. These phenomena may also be **transient** which immediately defeats the purpose of a characteristic heartbeat.\n","\n","\n","\n","The Wavelet decomposition will allows us to filter the original signal in an adaptitive way. It allows one to make a trade-off between frequency and time resolution as a function of scale.\n","\n","The Discrete Wavelet Transform is given by:\n","$$ \\text{DWT}_x^{\\psi}(m, n) = \\sum_{t} x(t) \\psi^*_{m, n}(t) = \\sum_{t} x(t) \\psi^*\\left(\\frac{t - n 2^m}{2^m}\\right)$$ using a precision of $2^{-m}$, or $m$ bits. $\\psi^*$ is the so-called *mother Wavelet function*.\n","\n","The following code provides visualization of the Haar Wavelet decomposition using 5 levels, i.e. a floating point precision of 5.\n","\n","We will use the sound you have previously filtered with the butterworth filters for illustrative purposes.\n","\n","\n","\n"],"metadata":{"id":"hYNkVp4_WWIu"}},{"cell_type":"code","source":["\n","n_levels = 5\n","coeffs = pywt.wavedec(filtered,\n","                      wavelet='haar',\n","                      level=n_levels)\n","# Plot the original signal alongside the wavelet coefficients\n","plt.figure(figsize=(12, 12))\n","\n","# Plot the original noisy signal\n","plt.subplot(n_levels + 2, 1, 1)\n","plt.plot(filtered,\n","         color='green')\n","plt.title('Original Noisy Signal')\n","plt.grid(True)\n","\n","# Plot the approximation coefficients at the highest level\n","plt.subplot(n_levels + 2, 1, 2)\n","plt.plot(coeffs[0], color='blue')\n","plt.title(f'Haar Approximation Coefficients (Level {n_levels})')\n","plt.grid(True)\n","\n","# Plot the detail coefficients for each level\n","for i in range(1, n_levels + 1):\n","    plt.subplot(n_levels + 2, 1, i + 2)\n","    plt.plot(coeffs[i], color='red')\n","    plt.title(f'Haar Detail Coefficients (Level {n_levels - i + 1})')\n","    plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"FKBDuraPXEiN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1.3.a - Universal Thresholding\n","Since the DWT provides a multiresolution decomposition of the signal, we can use our **prior knowledge** that *most of the information is concentrated in the low frequencies* to mitigate noise in an adaptative fashion. Rembember, we may not want to get rid of all high frequency content necesseraly!\n","\n","\n","\n","Suppose that the detail coefficients at the **finest scale** are distributed according to a standard Gaussian scaled by $\\sigma$. Then, using [extreme value theory](https://nobel.web.unc.edu/wp-content/uploads/sites/13591/2019/11/Gaussian_Extremes-1.pdf), the largest detail coefficient is:\n","\n","$$\\max_{i=1, \\ldots, n} |D_i| \\approx O(\\sigma\\sqrt{2\\log n})$$\n","\n","The higher the frequency, the more sensitive to small perturbations our estimates will be, hence we are looking for a robust estimator of $\\sigma$. Typically one uses the **median absolute deviation** estimate of the Gaussian:\n","\n","$$\\sigma \\approx \\frac{\\text{median}(|D|)}{0.6745}$$\n","\n","\n","\n","Now, we only need to define the thresholding function. We will implement the following soft-thresholding:\n","\n","$$\\hat{D}_j = \\text{sign}(D_j) \\cdot \\max(|D_j| - \\lambda, 0)$$\n","\n","where $\\lambda = \\sigma\\sqrt{2\\log n}$. This effectively zeroes out coefficients smaller or equal to $\\lambda$ and shifts the remaining $D_j$s towards 0 by $\\lambda$.\n","\n","\n","Implement a function that receives the recordings and outputs the a filtered version of the signal. Use `PyWavelets` package to do so, using `pywt.wavedec`to decompose the signal and `pywt.waverec` to reconstruct the signal (after applying universal thresholding to the coefficients).\n","\n","Run the cell below to see an example"],"metadata":{"id":"8sSSgJAcbJl3"}},{"cell_type":"code","source":["def soft_threshold(coeff, threshold):\n","        return np.sign(coeff) * np.maximum(np.abs(coeff) - threshold, 0)\n","\n","n_levels = 5\n","coeffs = pywt.wavedec(filtered, wavelet='haar', level=n_levels)\n","# Plot the original signal alongside the wavelet coefficients\n","plt.figure(figsize=(12, 12))\n","\n","# Define a threshold value (e.g., universal threshold)\n","sigma = np.median(np.abs(coeffs[-1])) / 0.6745  # Estimating noise level\n","threshold = sigma * np.sqrt(2 * np.log(len(filtered)))\n","\n","# Apply soft thresholding to the detail coefficients\n","coeffs_thresholded = [coeffs[0]]  # Keep approximation coefficients unchanged\n","for coeff in coeffs[1:]:\n","    coeffs_thresholded.append(soft_threshold(coeff, threshold))\n","\n","\n","# Plot the original noisy signal\n","plt.subplot(n_levels + 2, 1, 1)\n","plt.plot(filtered, color='green')\n","plt.title('Original Noisy Signal')\n","plt.grid(True)\n","\n","# Plot the approximation coefficients at the highest level\n","plt.subplot(n_levels + 2, 1, 2)\n","plt.plot(coeffs[0], color='blue')\n","plt.title(f'Haar Approximation Coefficients (Level {n_levels})')\n","plt.grid(True)\n","\n","# Plot the detail coefficients for each level\n","for i in range(1, n_levels + 1):\n","    plt.subplot(n_levels + 2, 1, i + 2)\n","    plt.plot(coeffs[i], color='red', label='original')\n","    plt.plot(coeffs_thresholded[i], color='purple', label='after universal threshold')\n","    plt.title(f'Haar Detail Coefficients (Level {n_levels - i + 1})')\n","    plt.grid(True)\n","    plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"OX5oy4gTbO4m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 - Visualize the original vs processed waveforms\n","Run the cell below to see how our signal-processing strategy affects the waveform."],"metadata":{"id":"svRkjUMwcN_H"}},{"cell_type":"code","source":["def plot_denoised_signal(dataset, denoised_sounds, sample_idx):\n","    noisy_signal = dataset[sample_idx]['recording']['array']\n","    denoised_signal = denoised_sounds[sample_idx]['recording']['array']\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(noisy_signal, label='Noisy Signal')\n","    plt.plot(denoised_signal, label='Denoised Signal', linewidth=2)\n","    plt.legend()\n","    plt.title(f'Signal denoising after denosing')\n","    plt.show()\n","\n","# Inspect the results on a sample\n","sample_idx = # choose sample idx\n","plot_denoised_signal(original, processed, sample_idx)"],"metadata":{"id":"p_9gWnK7cdHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 2 - Deep Learning and Model serving\n","\n","We have prepared a set of files with a set of features pre-extracted from the processed dataset.\n","\n","Specifically, we extracted amplitude and homomorphic envelograms and subsample the signals to from 4000 Hz to 50 Hz.\n","\n","Details can be found on most papers in the literature such as [this one](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242001)."],"metadata":{"id":"J9NLU-xicxSp"}},{"cell_type":"code","source":["dataset_dict = load_dataset(\"miguellmartins/circor-digiscope-physionet22-tutorial\")"],"metadata":{"id":"VmjKs89zWD53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxGmXrHMdXr7","executionInfo":{"status":"ok","timestamp":1741348509922,"user_tz":0,"elapsed":9,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"e44ae9cf-be2b-4294-b983-809961b70508"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['filename', 'recording', 'recording_label', 'heart_state_labels', 'amplitude_env', 'homomorphic_env', 'identifier'],\n","        num_rows: 2683\n","    })\n","    dev: Dataset({\n","        features: ['filename', 'recording', 'recording_label', 'heart_state_labels', 'amplitude_env', 'homomorphic_env', 'identifier'],\n","        num_rows: 337\n","    })\n","    val: Dataset({\n","        features: ['filename', 'recording', 'recording_label', 'heart_state_labels', 'amplitude_env', 'homomorphic_env', 'identifier'],\n","        num_rows: 343\n","    })\n","})"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["\n","We will be using a patch size $P$ of samples, so for a sample rate of 50Hz, the window spans roughly 1.3 seconds.\n","\n","We will processing the sounds patch-by-patch, so we will be discarding recordings that do not span at least one patch $P$.\n","\n","Run the following code to apply these steps to each dataset.\n","\n","Note that if you have more than 2 envelograms from Tutorial 1 you need to change `nch` (i.e., number of channels) accordingly.\n","\n","We recommend starting with `stride=32`, but you can revisit this part of the tutorial later and adapt all parameters to your liking."],"metadata":{"id":"P9Ghtm-ydkcG"}},{"cell_type":"code","source":["PATCH_SIZE = 64\n","NUMBER_CHANNELS = 2\n","NUMBER_CLASSES = 4\n","STRIDE = 32\n","BATCH_SIZE = 32"],"metadata":{"id":"w_jdBGwDWFVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some of the sounds might be too short for the above configuration. We will have to preemptively filter them before we move forward with training."],"metadata":{"id":"wdUdECzWd1iq"}},{"cell_type":"code","source":["def filter_datasets(dataset_dict: DatasetDict, patch_size: int = 64):\n","  _filter_small = lambda x: len(x) >= patch_size\n","  for split in dataset_dict:\n","    # We only to feature by one of the envelopes since they both have same length\n","    dataset_dict[split] = dataset_dict[split].filter(lambda x: len(x['amplitude_env']) >= patch_size)\n","  return dataset_dict\n","\n","\n","dataset_dict = filter_datasets(dataset_dict)"],"metadata":{"id":"Y83vQqqWWJnA","executionInfo":{"status":"ok","timestamp":1741348738081,"user_tz":0,"elapsed":11454,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["c8279d3df23240b69387b5d4e9b0d781","253c2bb18f9f49bf8ee5cdef9816e20c","2284145f93d340479087fb0044b6181d","598f5fa7960846ac91f4117c261e9ce3","b058296187434d7db6431b8b09241437","07a2f46c78274d07929a65519b72ee44","ae725c5529d54fb2b2b41e80ee09e2a9","c5fa669c4b4e40dba2a02350a561fa07","62660b7ad096494f990ce10dabdc1fca","762c0d0fb1df47f2a5a00563bfe03fdf","d5d8bd60e40240a19e111aa734b22ecb","73a7c9f877944e15b70756091589788e","125463dfa5b84794ab1be57a99aaf29b","878f8fe1c9b74f5c9a3ed0d7cb79fcd5","3697104b569944acac3ccfd785a270a2","944acf19c2044da189303097b88c546e","623cead1203e4cf8aaacf4556280dd25","533ab7c666d64910a62c858557b8b0b2","e356d91d85a142b7b4726108746250f6","50903b60b9b94334903099b3af031072","3f8b55afbaaa4167b0900d3d5f4da7ca","73b539d7aced4ddca1bd581e3823f6a3","2a621399936149588b9e64755438b400","03ada529c952459d928a78f6f82f6420","fdaa5f6fb7b94180be5055cae8f7b1cc","373e4a2e09de44f985b1ca22a9df99be","0c213786dd1e48fb95994f6e15f60b95","32a6166af9f54c3c8246844024c75d94","c5b02240e2fa46da847d962c2248b3b3","18446fd583514bf7a28bff26061d7e72","48926d629ac4489aab7ddc1513bfecc5","bae0058a97c44b1089a5cec80afc32c5","5003d6934f3a41f58faded828ea74840"]},"outputId":"6119eb4f-75b2-4571-a6c5-576c57c2e6b7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/2683 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8279d3df23240b69387b5d4e9b0d781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/337 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a7c9f877944e15b70756091589788e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/343 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a621399936149588b9e64755438b400"}},"metadata":{}}]},{"cell_type":"markdown","source":["The heart state labels are also ordinal. We will be changing the enconding to one-hot."],"metadata":{"id":"wecpSsk1ePxR"}},{"cell_type":"code","source":["def one_hot_encoding(num_classes: int = 4):\n","  one_hot_labels = np.eye(num_classes)\n","  def _one_hot_encoding(example):\n","    example[\"heart_state_labels\"] = one_hot_labels[np.array(example['heart_state_labels']) - 1]\n","    return example\n","  return _one_hot_encoding\n","\n","\n","for split in dataset_dict:\n","  dataset_dict[split] = dataset_dict[split].map(one_hot_encoding(num_classes=NUMBER_CLASSES))"],"metadata":{"id":"Y4RUi7xtWMUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The U-Net takes as input patches of a given size $P$.\n","Note that our sounds are downsampled to 50Hz by now (80$\\times$ smaller than the original 4KHz). However, loading all sound patches, especially if they overlap, may have unrealistic V-RAM requirements for most scenarios.\n","\n","With that in mind, we will have to compute our sound patches dynamically, i.e. online during training or inference.\n","\n","We make use of the [HuggingFace Dataset API](https://huggingface.co/docs/hub/datasets-overview) to build a streamable dataset that can than be instantiated as a [TensorFlow Dataset](https://www.tensorflow.org/guide/data) to serve our U-Net.\n","Please inspect the following class `PatchIterableDataset` which will handle all the [ETL](https://tinyurl.com/527fak67) for our deep learning data pipeline.\n","\n","Generators use the yield statement to produce a series of values, \"pausing\" the function each time a yield is encountered and resuming the execution in the next iteration. This makes them memory-efficient because they only produce items as needed, unlike lists that store all items in memory. Naturally, this solution is slower than pre-computing all patches."],"metadata":{"id":"ncZ8AtpVelgd"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from torch.utils.data import IterableDataset, DataLoader\n","\n","class PatchIterableDataset(IterableDataset):\n","    \"\"\"\n","    Iterates over a Hugging Face dataset, chunking frames into (patch_size, 2).\n","    Does NOT store the entire dataset in memory, only a buffer of length patch_size.\n","    \"\"\"\n","    def __init__(self, hf_dataset,\n","                 patch_size: int,\n","                 stride: int):\n","        \"\"\"\n","        Args:\n","            hf_dataset: A Hugging Face (Iterable)Dataset or anything else iterable\n","                        where each item is a dict with 'features' -> shape (T_i, 2).\n","            patch_size: Number of frames to accumulate before yielding a chunk.\n","            drop_incomplete_chunk: If True, discard leftover frames if they're\n","                                   less than patch_size at the end.\n","        \"\"\"\n","        super().__init__()\n","        self.dataset = hf_dataset\n","        self.patch_size = patch_size\n","        self.stride = stride\n","\n","    def __iter__(self):\n","        buffer = []\n","        # Stream over the original dataset, example by example:\n","        for example in self.dataset:\n","            amp_env, homo_env, label = example['amplitude_env'], example['homomorphic_env'], example['heart_state_labels']\n","            amp_env = np.array(amp_env)\n","            homo_env = np.array(homo_env)\n","            label = np.array(label)\n","                # Combine the two feature columns along a new dimension => shape: (time, 2)\n","            num_samples = len(homo_env)\n","            sound = np.stack([amp_env, homo_env], axis=-1)\n","            num_windows = int((num_samples - self.patch_size) / self.stride) + 1\n","            for window_idx in range(num_windows):\n","                patch_start = window_idx * self.stride\n","                yield sound[patch_start:patch_start + self.patch_size, :], label[patch_start: patch_start + self.patch_size, :]\n","\n","            window_remain = num_samples - self.patch_size\n","            if window_remain % self.stride > 0:\n","                yield sound[window_remain:, :], label[window_remain:, :]"],"metadata":{"id":"VaIZHT49WQ1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["patch_dataset_train = PatchIterableDataset(dataset_dict['train'],\n","                                           patch_size=PATCH_SIZE,\n","                                           stride=STRIDE)\n","\n","patch_dataset_dev = PatchIterableDataset(dataset_dict['dev'],\n","                                           patch_size=PATCH_SIZE,\n","                                           stride=STRIDE)\n","\n","patch_dataset_val = PatchIterableDataset(dataset_dict['val'],\n","                                           patch_size=PATCH_SIZE,\n","                                           stride=STRIDE)"],"metadata":{"id":"9MCtGWQBXGrO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"s0x_Ym4Kl-2y"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def get_tf_dataset(patch_dataset, number_channels, number_classes, patch_size, batch_size, cache=False):\n","  gen_fn = lambda: ((x, y) for (x,y) in patch_dataset)\n","  tf_ds = tf.data.Dataset.from_generator(\n","      generator=gen_fn,\n","      output_signature=(\n","          tf.TensorSpec(shape=(patch_size, number_channels), dtype=tf.float32),    # sound shape: (time, 2)\n","          tf.TensorSpec(shape=(patch_size, number_classes), dtype=tf.float32)       # label shape: (time,)\n","      )\n","  )\n","  if cache:\n","    tf_ds = tf_ds.cache()\n","  tf_ds = tf_ds.batch(batch_size)\n","  tf_ds = tf_ds.prefetch(tf.data.AUTOTUNE)\n","  return tf_ds\n"],"metadata":{"id":"IlMlBV4_WXEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following code to create the Tensorflow Dataset objects using our custom data generator. We will be using `.cache()` and `.prefetch()` from the [tf.Data API](https://www.tensorflow.org/guide/data_performance) to minimize the overhead of computing each batch of data online."],"metadata":{"id":"7DutAgJ_fQlk"}},{"cell_type":"code","source":["train = get_tf_dataset(patch_dataset=patch_dataset_train,\n","                       number_channels=NUMBER_CHANNELS,\n","                       number_classes=NUMBER_CLASSES,\n","                       patch_size=PATCH_SIZE,\n","                       batch_size=BATCH_SIZE,\n","                       cache=True) # you could cache all dataframes but you need more compute for that\n","\n","dev = get_tf_dataset(patch_dataset=patch_dataset_dev,\n","                       number_channels=NUMBER_CHANNELS,\n","                       number_classes=NUMBER_CLASSES,\n","                       patch_size=PATCH_SIZE,\n","                       batch_size=BATCH_SIZE)\n","\n","val = get_tf_dataset(patch_dataset=patch_dataset_val,\n","                       number_channels=NUMBER_CHANNELS,\n","                       number_classes=NUMBER_CLASSES,\n","                       patch_size=PATCH_SIZE,\n","                       batch_size=BATCH_SIZE)"],"metadata":{"id":"N8uW-WibX949"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Introduced by [Ronneberg et al.](https://arxiv.org/abs/1505.04597) in 2015, the U-Net is ubiquitous in biomedical signal and image processing tasks.\n","\n","Although its efficacy is a result of several advancements of modern deep learning and optimization techniques, one of its characteristic architetural patterns are the skip connections from the encoder to the decoder.\n","\n","The idea is that low-resolution information is important for medical domains, which can be complementary to the semantical rich features in the decoder.\n","\n","Inspect and run the code below of a template for a simple U-Net adapted to process signals instead of images; using 1D instead of 2D primitives."],"metadata":{"id":"xLgDxSv_fKnB"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, concatenate\n","\n","def example_unet(patch_size, nch, dropout=0.0):\n","    inputs = tf.keras.layers.Input(shape=(patch_size, nch))\n","    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(inputs)\n","    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(conv1)\n","    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n","    pool1 = tf.keras.layers.Dropout(dropout)(pool1)\n","\n","    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(pool1)\n","    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv2)\n","\n","    up_prep = tf.keras.layers.UpSampling1D(size=2)(conv2)\n","\n","    up = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(8, 2, padding='same')(up_prep), conv1], axis=2)\n","    up = tf.keras.layers.Dropout(dropout)(up)\n","    convout = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(up)\n","    convout = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(convout)\n","\n","    output_layer = tf.keras.layers.Conv1D(4, 1, activation='softmax')(convout)\n","\n","    model = tf.keras.Model(inputs=[inputs], outputs=[output_layer])\n","    return model\n"],"metadata":{"id":"Pn0PNfH2ksD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = example_unet(PATCH_SIZE, NUMBER_CHANNELS)\n","print(model.summary())"],"metadata":{"id":"MKG4eotlmvps"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instantiate your U-Net model and select a set of adequate metrics to track.\n","\n","Early stopping on validation set is performed using `ModelCheckpoint` on `val_loss`.\n","\n","We will be using the Adam Gradient Descent algorithm.\n","If you have access to a GPU run this code for 50 epochs.\n","If not, 10 epochs should still give you a sufficiently capable model to complete this tutorial"],"metadata":{"id":"Q5yIvxk2fs8l"}},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","EPOCHS = 50 # your epochs\n","learning_rate = 1e-4  # your lr\n","model =  example_unet(PATCH_SIZE, NUMBER_CHANNELS)  # use your unet_here\n","checkpoint_path = 'unet_weights/unet.keras'\n","model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy',\n","                  metrics=['categorical_accuracy', 'precision', 'recall'])\n","model_checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n","                                   monitor='val_loss',\n","                                   save_best_only=True)"],"metadata":{"id":"rjzQ_vRwysfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train,\n","                    validation_data=dev,\n","                    epochs=EPOCHS,\n","                    verbose=1,\n","                    shuffle=True,\n","                    callbacks=[model_checkpoint])\n","\n","model.load_weights(checkpoint_path)"],"metadata":{"id":"8zYf8-iCfl5h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us perform inference on the external validation dataset."],"metadata":{"id":"b1KB6LengDtN"}},{"cell_type":"code","source":["predictions_train = model.predict(train)\n","predictions_dev = model.predict(dev)\n","predictions_test = model.predict(val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4SS7msEnB1t","executionInfo":{"status":"ok","timestamp":1741349725954,"user_tz":0,"elapsed":13130,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"5047bd43-9a71-4f5d-a3ef-de30b70b55c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: ['keras_tensor_28']\n","Received: inputs=Tensor(shape=(32, 64, 2))\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n","\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step\n","\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step\n"]}]},{"cell_type":"markdown","source":["`predictions_test` has the predictions on the test-set patch-wise in a patient agnostic fashion.\n","\n","We now need to associate the appropriate outputs for each recording in an adequate fashion. Moreover, the fact that we used an overalpping slidding window means that we will have several heart state estimates for a given time $t$.\n","\n","We will be averaging the U-Net's predictions across the patch dimension. The strategy resembles the `__iter__` method of `PatchIterableDataset`. Let $N_{P_i} = \\lceil  \\frac{T_i - P}{\\tau} \\rceil$\n","\n","$$\\tilde{\\mathbf{y}}_i(t, \\tau) = \\frac{1}{N_{P_i}}\\sum_{t=1}^{N_{P_i}} \\text{U-Net}(\\mathbf{x}_i(t, \\tau)| \\theta^*)$$\n","So $\\tilde{\\mathbf{y}}_i$ should have shape $(T_i, 4)$.\n","\n","Inspect the function `process_unet_predictions`. It should return the predicted label sequence (in ordinal form, not one-hot-encoded), and the corresponding probabilty estimates."],"metadata":{"id":"NCxMDXaigI7R"}},{"cell_type":"code","source":["def process_unet_predictions(preds, dataframe, patch_size, stride):\n","  num_observations = len(dataframe)\n","  output_probs = np.ndarray(shape=(num_observations), dtype=np.ndarray)\n","  output_seqs = np.ndarray(shape=(num_observations), dtype=np.ndarray)\n","  preds_idx = 0\n","  for idx, sample in tqdm(enumerate(dataframe)):\n","    sound = sample['heart_state_labels']\n","    sound_duration = len(sound)\n","    # number of patches associated to this sound\n","    number_patches = int((sound_duration - patch_size) / stride)\n","    if (sound_duration - patch_size) % stride > 0:\n","      number_patches += 1\n","    prob_sound = np.zeros((number_patches, sound_duration, 4))\n","    for i in range(number_patches):\n","      prob_sound[i, i * stride:i * stride + patch_size, :] = preds[preds_idx, :, :]\n","      preds_idx += 1\n","    if (sound_duration - patch_size) % stride > 0:\n","      prob_sound[number_patches - 1, sound_duration - patch_size:, :] = preds[preds_idx, :, :]\n","      preds_idx +=1\n","\n","    probs_patch = tf.reduce_sum(prob_sound, axis=0)\n","    probs_patch_normalized = probs_patch /  (tf.reduce_sum(prob_sound, axis=[0,2])[:, tf.newaxis] + 1e-12)\n","    output_probs[idx] = probs_patch_normalized\n","    output_seqs[idx] = tf.argmax(probs_patch_normalized, axis=1)\n","  return output_probs, output_seqs"],"metadata":{"id":"7CVX00f8nlKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs, predictions = process_unet_predictions(predictions_test, dataset_dict['val'], PATCH_SIZE, STRIDE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-e_UsPWqmNT","executionInfo":{"status":"ok","timestamp":1741349728651,"user_tz":0,"elapsed":2325,"user":{"displayName":"Miguel Martins","userId":"07021340133375705975"}},"outputId":"63bceabc-f4a5-4ae7-f8ba-f1ca053c6d39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["325it [00:02, 138.88it/s]\n"]}]},{"cell_type":"markdown","source":["The U-Net outputs estimates patch-by-patch, meaning it can output invalid heart sequences, e.g. going from S1 directly to S2 (we are assuming that screening is not interrupted here).\n","\n","Implement a deconding function that takes the sequence predicted patch-by-patch that you recovered using `process_unet_predictions` and processes the output to be valid. You can change the output according to any criteria you want.\n","\n","The function you implement should pass the unit test in the cell bellow."],"metadata":{"id":"cEt8GDCjgpoo"}},{"cell_type":"code","source":["def your_deconding_function(seq, num_states=4):\n","  \"\"\"your code here\"\"\"\n","  return seq\n","\n","test_seq = np.array([1, 1, 1, 2, 1, 1, 2, 2, 3, 3, 0, 3])\n","exp_seq = np.array([1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 0, 0])\n","\n","out_seq = your_deconding_function(test_seq)\n","assert np.all(exp_seq == out_seq)"],"metadata":{"id":"tq8nHhkGra2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","test_labels = dataset_dict['val']['heart_state_labels']\n","ground_truth = np.array([np.argmax(y, axis=1) for y in tqdm(test_labels)], dtype=object)\n","predictions = np.array([your_deconding_function(prediction.numpy()) for prediction in predictions], dtype=object)"],"metadata":{"id":"UP01t5y2skfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will follow [Schmidt et al.](https://iopscience.iop.org/article/10.1088/0967-3334/31/4/004/pdf). A sound is true positive (TP) or correctly located if\n","the middle of the detected sound is closer than 60 ms to the middle of a similarly predefined sound, all other detected sounds were defined as false positive (FP).\n","\n","Sensitivity is defined as:\n","\\begin{equation}\n","\\text{Sensitivity} = \\frac{\\text{number of TP sounds}}{\\text{total number of S1 and S2 sounds}}\n","\\end{equation}\n","\n","and positive predictivity ($P_+$):\n","\\begin{equation}\n","P_+ = \\frac{\\text{number of TP sounds}}{\\text{number of TP sounds} + \\text{number of FP sounds}}.\n","\\end{equation}\n","\n","These metrics are tricky to implement so we provide them beforehand (the authors may or may not had a little help from o1-preview for this step :))"],"metadata":{"id":"fq7YHBSBg1Mw"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score\n","def extract_state_runs(labels, desired_states):\n","    \"\"\"\n","    Extract continuous runs of the desired states from labels.\n","\n","    Args:\n","        labels: numpy array of labels.\n","        desired_states: set of desired state values.\n","\n","    Returns:\n","        A list of dictionaries with keys:\n","            'start': start index of the run\n","            'end': end index of the run (inclusive)\n","            'midpoint': midpoint index of the run\n","            'state': the state value (0 or 2)\n","    \"\"\"\n","    # Ensure labels is a 1D array\n","    labels = np.asarray(labels).flatten()\n","\n","    runs = []\n","    N = len(labels)\n","    in_run = False\n","    run_start = 0\n","    run_state = None\n","\n","    for i in range(N):\n","        label_i = labels[i]\n","        # If label_i is an array (e.g., from a structured array), extract scalar\n","        if isinstance(label_i, np.ndarray):\n","            label_i = label_i.item()\n","        if label_i in desired_states:\n","            if not in_run:\n","                # Start of a new run\n","                in_run = True\n","                run_start = i\n","                run_state = label_i\n","        else:\n","            if in_run:\n","                # End of the run\n","                run_end = i - 1\n","                midpoint = (run_start + run_end) // 2\n","                runs.append({\n","                    'start': run_start,\n","                    'end': run_end,\n","                    'midpoint': midpoint,\n","                    'state': run_state\n","                })\n","                in_run = False\n","                run_state = None\n","    # Check if we're still in a run at the end\n","    if in_run:\n","        run_end = N - 1\n","        midpoint = (run_start + run_end) // 2\n","        runs.append({\n","            'start': run_start,\n","            'end': run_end,\n","            'midpoint': midpoint,\n","            'state': run_state\n","        })\n","    return runs\n","\n","def compute_ppv_sensitivity(ground_truth, predictions, sample_rate, threshold=60e-3):\n","    \"\"\"\n","    Compute PPV and sensitivity for states 0 and 2.\n","\n","    Args:\n","        ground_truth: numpy array of ground truth labels.\n","        predictions: numpy array of predicted labels.\n","        sample_rate: sampling rate in Hz.\n","\n","    Returns:\n","        ppv: Positive Predictive Value.\n","        sensitivity: Sensitivity (Recall).\n","    \"\"\"\n","    # Ensure ground_truth and predictions are 1D arrays\n","    ground_truth = np.asarray(ground_truth).flatten()\n","    predictions = np.asarray(predictions).flatten()\n","\n","    # Desired states\n","    desired_states = {0, 2}\n","\n","    # Maximum distance in samples (treshold in seconds vs fs)\n","    max_distance_samples = int(threshold * sample_rate)\n","\n","    # Extract runs from ground truth and predictions\n","    gt_runs = extract_state_runs(ground_truth, desired_states)\n","    pred_runs = extract_state_runs(predictions, desired_states)\n","\n","    # Get midpoints and states\n","    gt_midpoints = np.array([run['midpoint'] for run in gt_runs])\n","    gt_states = np.array([run['state'] for run in gt_runs])\n","\n","    pred_midpoints = np.array([run['midpoint'] for run in pred_runs])\n","    pred_states = np.array([run['state'] for run in pred_runs])\n","\n","    # Initialize matches\n","    matched_gt_indices = set()\n","    matched_pred_indices = set()\n","\n","    # Build potential matches\n","    potential_matches = []\n","    for i, (p_mid, p_state) in enumerate(zip(pred_midpoints, pred_states)):\n","        for j, (gt_mid, gt_state) in enumerate(zip(gt_midpoints, gt_states)):\n","            if gt_state == p_state:\n","                distance = abs(p_mid - gt_mid)\n","                if distance <= max_distance_samples:\n","                    potential_matches.append((i, j, distance))\n","\n","    # Sort potential matches by distance\n","    potential_matches.sort(key=lambda x: x[2])\n","\n","    # Perform matching\n","    TP = 0\n","    for i, j, d in potential_matches:\n","        if i not in matched_pred_indices and j not in matched_gt_indices:\n","            matched_pred_indices.add(i)\n","            matched_gt_indices.add(j)\n","            TP += 1\n","\n","    # Compute FP and FN\n","    total_pred = len(pred_midpoints)\n","    total_gt = len(gt_midpoints)\n","    FP = total_pred - len(matched_pred_indices)\n","    FN = total_gt - len(matched_gt_indices)\n","\n","    # Compute PPV and Sensitivity\n","    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n","\n","    return ppv, sensitivity"],"metadata":{"id":"BDsQhHlIvbSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_schmidt_metrics(ground_truth, sequences, sampling_rate):\n","  ppvs, sensitivities, accuracies = [], [], []\n","  for i in tqdm(range(len(ground_truth))):\n","    ppv, sensitivity = compute_ppv_sensitivity(ground_truth[i],\n","                                               sequences[i],\n","                                               sampling_rate)\n","    ppvs.append(ppv)\n","    sensitivities.append(sensitivity)\n","    accuracies.append(accuracy_score(ground_truth[i], sequences[i]))\n","  return np.array(ppvs), np.array(sensitivities), np.array(accuracies)\n","\n","\n","ppv, sens, acc = compute_schmidt_metrics(ground_truth, predictions, sampling_rate=50)"],"metadata":{"id":"Q2I8i7xuhB18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(ppv), np.mean(sens), np.mean(acc)"],"metadata":{"id":"N7FxYPAJvvON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspect a result where your metrics had satisfactory performance and one where the result was less positive. Discuss the differences."],"metadata":{"id":"fa-peH2LhKfG"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","def visualize_predictions(ground_truth, seqs, idx):\n","\n","  # Define the window width in terms of seconds and convert to the corresponding sample range\n","  window_duration = 0.1  # 0.06 ms in seconds\n","  sample_interval = 1 / 50  # Time per sample in seconds (20 ms per sample at 50 Hz)\n","\n","  # Calculate the equivalent width in terms of sample indices (will be <1)\n","  window_width_samples = window_duration / sample_interval\n","\n","  # Create the plot\n","  plt.figure(figsize=(24, 6))\n","\n","  # Plot ground truth and predictions with discrete markers and dotted lines\n","  plt.plot(ground_truth[idx], 'o--', label='Ground Truth', markersize=6)\n","  plt.plot(seqs[idx], 'o--', color='red', label='Predictions', markersize=6)\n","\n","\n","  # Set labels and legend\n","  plt.title(f'Signal at idx {idx} with 0.06 ms reference window')\n","  plt.xlabel('Sample Index')\n","  plt.ylabel('Amplitude')\n","  plt.legend()\n","  plt.grid(True)\n","  plt.show()\n","\n"],"metadata":{"id":"XAnmF5LKw6DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 0\n","visualize_predictions(ground_truth, predictions, idx)\n","ppv[idx], sens[idx], acc[idx]"],"metadata":{"id":"4BqzSVd-xJCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 100\n","visualize_predictions(ground_truth, predictions, idx)\n","ppv[idx], sens[idx], acc[idx]"],"metadata":{"id":"uSWUJNvExL_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although our model is far from perfect, we can still see that is indeed learning the rhythmic characteritics of the heart sound sequence.\n","\n","What you would do this perliminary model?\n","\n","\n","*  Do you think the filter-extraction plays a pivotal role? What if only used the original waveform without envelograms?\n","*  We know how the heart sounds are described statistically (at least during screening). They have a strong Markovian character. Could we somehow use this to make our model aware of this process?\n","* Do you think we could have a smarter way of decoding the heart states?\n","*   What architectural changes could be implemented? How about recurrent networks or Transformers?\n","\n","Feel free to reach out if you want to discuss these questions!\n","Contacts can be found at: https://miguelmartins.github.io/\n","\n","\n"],"metadata":{"id":"5-70UqFBho6C"}}]}