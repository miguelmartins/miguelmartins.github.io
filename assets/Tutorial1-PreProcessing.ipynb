{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0R2LlpW4MiB"
   },
   "source": [
    "## Preliminaries\n",
    "Access the following file in google drive:\n",
    "https://drive.google.com/drive/folders/1JelLO7mhzo-uttHvYrwLIxFmipceRrLP?usp=sharing\n",
    "\n",
    "Right-click it then: select Organize, Add Shortcut.\n",
    "Enable also high-RAM runtime via `Runtime > Change runtime` type in the menu. Then select `High-RAM` in the Runtime shape toggle button.\n",
    "\n",
    "This will allow you to read the files using the provided code in this tutorial!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "_VYEiq1AUdPg",
    "outputId": "1e67940b-5c6b-484f-ff3f-dd64923eddb0"
   },
   "outputs": [],
   "source": [
    "!pip install PyWavelets speechpy\n",
    "import os\n",
    "import librosa\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import speechpy\n",
    "import scipy.io as sio\n",
    "import scipy.signal\n",
    "import re\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie-yVUrRjFsX"
   },
   "source": [
    "#Deep Learning Techniques for Digital Cardiac Auscultation Analysis\n",
    "\n",
    "In this tutorial, we will explore the use of signal processing and deep learning techniques to analyze heart sound signal, i.e., phonocardiograms (PCGs), collected with a digital stethoscope.\n",
    "\n",
    "Heart sounds, contain valuable information about the condition of the heart and are commonly used by clinicians to detect cardiac abnormalities. The advancement of deep learning has opened new possibilities for automating the analysis of these sounds, allowing for more precise and scalable screening and monitoring tools.\n",
    "\n",
    "This tutorial is divided into two hands-on parts. In the first part, we will focus on the fundamentals of handling heart sound data, focusing on the processes of data loading, visualization, and preprocessing.\n",
    "\n",
    "The second part of the tutorial is dedicated to applying deep learning models to heart sound analysis, with a focus on segmentation and classification. In this section, you will explore methods to segment heart sounds into their fundmental components (e.g., S1 sound, systole, S2 sound, and diastole) and classify them to identify the presence of murmurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Ot8Uc-aUdPh",
    "outputId": "850fce7b-3d73-4df9-e2c9-f2abb8e5c857"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('drive/MyDrive/circor-dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6JBn7i8V_Zp"
   },
   "source": [
    "# Part 1. Visualization\n",
    "\n",
    "You have access to processed samples from the [2022 CirCor DigiScope dataset](https://physionet.org/content/circor-heart-sound/1.0.3/) already processed in the form of\n",
    "\n",
    "$$dataset[i, t]= [\\text{wavfile}_i, \\mathbf{x}_i, \\mathbf{y}_i, c_i], i=1, \\ldots, N,$$\n",
    "so that $\\text{wavfile}_i \\in \\Sigma^*$ (i.e., a string with the .wavfile name) , $\\mathbf{x}_i \\in \\mathbb{R}^{T_i}$, $\\mathbf{y}_i \\in \\{1,2,3,4\\}^{T_i}, c_i \\in \\{0, 1, 2\\}$. The sampling rate is 4000Hz, meaning that from $t$ to $(t+1)$ $1/4000$ seconds have passed. Note that each sound has a duration $T_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlRzTI-eUdPh"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('tutorialdf1.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2. Inspect dataset\n",
    "Inspect each column of the dataset"
   ],
   "metadata": {
    "id": "1uTD1A9J9FWh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_pandas = pd.DataFrame(dataset)\n",
    "dataset_pandas.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "O65E0gAy96bb",
    "outputId": "e22982b9-ba66-4796-da3f-cb65f33bc696"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Play close attention to column 0, i.e., the name of wavfile. It has the `patient identifier`, separated by `_`, and the auscultation location `PV, TV, AV, MV, Phc`. This will be important later!"
   ],
   "metadata": {
    "id": "X_VQne_n-Be8"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R66WfDuzWKHZ"
   },
   "source": [
    "## 1.1 Pick a random sound and use the below function to visualize the sound and its annotation\n",
    "\n",
    "Remember that each heart state is codified with categorical labels. So\n",
    "* 1: S1\n",
    "* 2: Systole\n",
    "* 3: S2\n",
    "* 4: Diastole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0M5iUn0WSe0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_sound_and_label(x, y, sampling_rate=4000):\n",
    "    assert len(x) == len(y)\n",
    "    number_of_samples = len(x)\n",
    "    # Time (duration) = T_i / sample_rate\n",
    "    time = np.arange(number_of_samples) / sampling_rate\n",
    "\n",
    "    # Plotting x and y together\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot x on the primary y-axis\n",
    "    ax1.plot(time, x, label='x (PCG)', color='b')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Amplitude of o', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Create a secondary y-axis for y\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.step(time, y, label='y (Heart Sattes)', color='r', where='post', linewidth=2)\n",
    "    ax2.set_ylabel('y (Labels)', color='r')\n",
    "    ax2.set_yticks([1, 2, 3, 4])\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    fig.suptitle('PCG Amplitude and Heart Sound Labels')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEANBUUWUdPj"
   },
   "outputs": [],
   "source": [
    "sample_idx = 0  # chose an idx i and visualize it\n",
    "sample = dataset[sample_idx]\n",
    "plot_sound_and_label(x=sample[1], y=sample[2], sampling_rate=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UIwEeJyeUdPj"
   },
   "source": [
    "## 1.2 Mel-frequency anaylsis\n",
    "Herein we will extract the Mel-frequency ceptrum and the MFCCs coefficients using `librosa`.\n",
    "\n",
    "View the MFCC and Melspectogram using the provided functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4bDsVuIMsUE"
   },
   "outputs": [],
   "source": [
    "def view_melspectogram(sound, sampling_rate=4000, n_mels=128):\n",
    "  fmax = sampling_rate / 2\n",
    "  mel_spectrogram = librosa.feature.melspectrogram(y=sound.astype(np.float32),\n",
    "                                                   sr=sampling_rate,\n",
    "                                                   n_mels=n_mels,\n",
    "                                                   fmax=fmax)\n",
    "\n",
    "  # Convert to decibel (dB) scale\n",
    "  mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "  # Plot the Mel spectrogram\n",
    "  plt.figure(figsize=(10, 4))\n",
    "  librosa.display.specshow(mel_spectrogram_db,\n",
    "                           sr=sampling_rate,\n",
    "                           x_axis='time',\n",
    "                           y_axis='mel',\n",
    "                           fmax=fmax)\n",
    "  plt.colorbar(format='%+2.0f dB')\n",
    "  plt.title('Mel Spectrogram')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "view_melspectogram(sound=sample[1], sampling_rate=4000)"
   ],
   "metadata": {
    "id": "jT0x-NRj_sAt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65-12p8hUdPk"
   },
   "outputs": [],
   "source": [
    "def get_mfcc(x,\n",
    "             *,\n",
    "              n_fft,\n",
    "              window_length,\n",
    "              hop_length,\n",
    "              n_mfcc,\n",
    "              fmin=25,\n",
    "              fmax=400,\n",
    "              sampling_rate=4000):\n",
    "    S = librosa.feature.melspectrogram(y=x.astype(np.float32).squeeze(),\n",
    "                                        n_fft=n_fft,\n",
    "                                        sr=sampling_rate,\n",
    "                                        win_length=window_length,\n",
    "                                        hop_length=hop_length,  # samples\n",
    "                                        fmin=fmin,\n",
    "                                        fmax=fmax,\n",
    "                                        window='hann')\n",
    "    # Convert to log scale (dB).\n",
    "    log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=n_mfcc)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_mfcc(x, hop_length, sampling_rate=4000):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(x,\n",
    "                             x_axis='time', sr=sampling_rate, hop_length=hop_length)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('MFCC')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "uc5nNfKxCiuq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxxFpwBY2V5s"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 4000\n",
    "x_mfcc = get_mfcc(sample[1],\n",
    "                  n_mfcc=8,\n",
    "                  n_fft=512,\n",
    "                  window_length=int(0.0125 * sampling_rate),  # 80 ms\n",
    "                  hop_length=int(0.025 * sampling_rate),  # frame-shift 20 ms => overlap 60 ms\n",
    "                  fmin=30,\n",
    "                  fmax=500)\n",
    "\n",
    "visualize_mfcc(x_mfcc,\n",
    "               hop_length=int(0.025 * sampling_rate),\n",
    "               sampling_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CivSbbJyUdPk"
   },
   "source": [
    "# 2. Pre-processing\n",
    "You will start to get your hands dirty here.\n",
    "We need to develop a pipeline for our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hBWOysDsUdPl"
   },
   "source": [
    "# 2.1.1 Band-pass filtering\n",
    "Select which low and high pass frenquecies we want to filter. We are specifically using [Butterworth filters](https://en.wikipedia.org/wiki/Butterworth_filter) for this purpose.\n",
    "\n",
    " Can you survey the literature for good options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IISclK_xXtXK"
   },
   "outputs": [],
   "source": [
    "def plot_filter_responses(low_pass_fs, high_pass_fs, sampling_rate=4000, filter_order=2):\n",
    "    \"\"\"\n",
    "    Plots the frequency responses of a highpass and a lowpass Butterworth filter.\n",
    "\n",
    "    Parameters:\n",
    "    - low_pass_fs: float, the cutoff frequency for the highpass filter (Hz)\n",
    "    - high_pass_fs: float, the cutoff frequency for the lowpass filter (Hz)\n",
    "    - sampling_rate: float, the sampling rate of the signals (Hz)\n",
    "    - filter_order: int, the order of the Butterworth filter (default is 2)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.signal import butter, sosfreqz\n",
    "\n",
    "    # Design the highpass filter\n",
    "    sos_hp = butter(N=filter_order, Wn=low_pass_fs, btype='highpass', analog=False, fs=sampling_rate, output='sos')\n",
    "    # Design the lowpass filter\n",
    "    sos_lp = butter(N=filter_order, Wn=high_pass_fs, btype='lowpass', analog=False, fs=sampling_rate, output='sos')\n",
    "\n",
    "    # Frequency response for the highpass filter\n",
    "    w_hp, h_hp = sosfreqz(sos_hp, fs=sampling_rate)\n",
    "    # Frequency response for the lowpass filter\n",
    "    w_lp, h_lp = sosfreqz(sos_lp, fs=sampling_rate)\n",
    "\n",
    "    # Plot the frequency response of both filters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for the highpass filter\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(w_hp, 20 * np.log10(np.abs(h_hp)), label='Highpass Filter')\n",
    "    plt.title('Highpass Filter Frequency Response')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude (dB)')\n",
    "    plt.grid()\n",
    "    plt.axvline(low_pass_fs, color='red', linestyle='--', label=f'Cutoff: {low_pass_fs} Hz')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for the lowpass filter\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(w_lp, 20 * np.log10(np.abs(h_lp)), label='Lowpass Filter')\n",
    "    plt.title('Lowpass Filter Frequency Response')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude (dB)')\n",
    "    plt.grid()\n",
    "    plt.axvline(high_pass_fs, color='red', linestyle='--', label=f'Cutoff: {high_pass_fs} Hz')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We suggest starting with a low-pass cutoff frequency of 100 Hz and high-pass cutoff frequency of 500 Hz.\n",
    "Visualize the filter responses using `plot_filter_responses`."
   ],
   "metadata": {
    "id": "3XH2HHM_C5_O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot_filter_responses(low_pass_fs=50, high_pass_fs=500)"
   ],
   "metadata": {
    "id": "W_UiD2RlC-Pn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6jRIyMzc8OE"
   },
   "source": [
    "## 2.1.1.Bonus - Can you search the literature for more adequate cutoff frequencies? Plot the results of your inquiries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3INsGdd1dKxq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQJN3QZ93pB5"
   },
   "source": [
    "## 2.1.1.a. Filter the dataset\n",
    "Iterate through all the samples in the dataset and apply the filters using the above-specified filters. Use the function `filter_sounds` with the desired low-pass and high-pass filter frequencies to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGcUrQtEUdPl"
   },
   "outputs": [],
   "source": [
    "def filter_sounds(data,\n",
    "                  low_pass_fs,\n",
    "                  high_pass_fs,\n",
    "                  sampling_rate=4000):\n",
    "    filtered_data = []\n",
    "    for recording in tqdm(data, 'Filtering recordings', total=len(data), leave=True):\n",
    "        sos_hp = scipy.signal.butter(N=2, Wn=low_pass_fs, btype='highpass', analog=False, fs=sampling_rate, output='sos')\n",
    "        sos_lp = scipy.signal.butter(N=2, Wn=high_pass_fs, btype='lowpass', analog=False, fs=sampling_rate, output='sos')\n",
    "        filtered = scipy.signal.sosfilt(sos_hp, recording)\n",
    "        filtered = scipy.signal.sosfilt(sos_lp, filtered)\n",
    "        filtered_data.append(filtered)\n",
    "    return np.array(filtered_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYafJJJeUdPl"
   },
   "outputs": [],
   "source": [
    "sounds = dataset[:, 1]\n",
    "filtered_sounds = filter_sounds(sounds, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFv_8xwvUdPl"
   },
   "outputs": [],
   "source": [
    "sample_filtered = dataset[sample_idx]\n",
    "plot_sound_and_label(x=sample[1], y=sample[2])\n",
    "plot_sound_and_label(x=sample_filtered[1], y=sample[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us assign the filtered sound to the respective dataset column.\n",
    "\n",
    "**Only run this cell when you are certain of your filtering startegy. If you make a mistake you might have to restart the notebook and run every cell**"
   ],
   "metadata": {
    "id": "FnAhRmtHEJBz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnhEcnHUUdPl"
   },
   "outputs": [],
   "source": [
    "dataset[:, 1] = filtered_sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9_YJ6TdMUdPl"
   },
   "source": [
    "## 2.1.2 Denoising -  Averaging Theory\n",
    "The section of denoising in this tutorial follows [Messer et al.](https://www.sciencedirect.com/science/article/pii/S0026269201000957).\n",
    "\n",
    "\n",
    "Suppose a Source $\\mathbf{S}$ is corrupted additively by i.i.d. Gaussian noise $\\epsilon_i$. We observe $\\mathbf{X}$ and not $\\mathbf{S}$ in our measurements such that:\n",
    "\n",
    "$$X_i = S + \\epsilon_i, i=1,\\ldots, N$$\n",
    "\n",
    "If one computes its variance:\n",
    "\n",
    "$$\\text{Var}\\left(\\mathbf{S} + \\frac{1}{N} \\sum_{j=1}^{N}  \\epsilon_j\\right) = \\text{Var}(\\mathbf{S}) + \\text{Var}\\left(\\frac{1}{N} \\sum_{j=1}^{N}  \\epsilon_j \\right) = \\text{Var}(\\mathbf{S}) + \\frac{T_i\\sigma^2}{N^2}= \\text{Var}(\\mathbf{S}) + \\frac{\\sigma^2}{N}$$\n",
    "\n",
    "one observes that the **standard deviation** of the random terms will shrink as $\\frac{\\sigma}{\\sqrt(N)}$.\n",
    "\n",
    "Due to the periodic nature of heart sounds and their stationarity (at least in a \"short\" period of time), we can think of $\\mathbf{S}$ to be the expected waveform of the heart cycle in a recording, i.e. the *characteristic heart cycle* of a patient.\n",
    "\n",
    "## 2.1.3 Wavevet Denoising\n",
    "\n",
    "In practice, the analysis of a characteristic heartbeat for most downstream\n",
    "applications is not very useful. Even under (quasi)-stationary assumptions, there may be other phenomena present in the signal such as murmurs that may occur in all states of the heart cycles and across several frequency bands. These phenomena may also be **transient** which immediately defeats the purpose of a characteristic heartbeat.\n",
    "\n",
    "\n",
    "\n",
    "The Wavelet decomposition will allows us to filter the original signal in an adaptitive way. It allows one to make a trade-off between frequency and time resolution as a function of scale.\n",
    "\n",
    "The Discrete Wavelet Transform is given by:\n",
    "$$ \\text{DWT}_x^{\\psi}(m, n) = \\sum_{t} x(t) \\psi^*_{m, n}(t) = \\sum_{t} x(t) \\psi^*\\left(\\frac{t - n 2^m}{2^m}\\right)$$ using a precision of $2^{-m}$, or $m$ bits. $\\psi^*$ is the so-called *mother Wavelet function*.\n",
    "\n",
    "The following code provides visualization of the Haar Wavelet decomposition using 5 levels, i.e. a floating point precision of 5. We will analyse an actual heart sound of our dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHW6gIoZePYS"
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "max_duration = 1000\n",
    "n_levels = 5\n",
    "coeffs = pywt.wavedec(filtered_sounds[0][:max_duration], wavelet='haar', level=n_levels)\n",
    "# Plot the original signal alongside the wavelet coefficients\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot the original noisy signal\n",
    "plt.subplot(n_levels + 2, 1, 1)\n",
    "plt.plot(filtered_sounds[0][:max_duration], color='green')\n",
    "plt.title('Original Noisy Signal')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the approximation coefficients at the highest level\n",
    "plt.subplot(n_levels + 2, 1, 2)\n",
    "plt.plot(coeffs[0], color='blue')\n",
    "plt.title(f'Haar Approximation Coefficients (Level {n_levels})')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the detail coefficients for each level\n",
    "for i in range(1, n_levels + 1):\n",
    "    plt.subplot(n_levels + 2, 1, i + 2)\n",
    "    plt.plot(coeffs[i], color='red')\n",
    "    plt.title(f'Haar Detail Coefficients (Level {n_levels - i + 1})')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrH2QfGLdtOv"
   },
   "source": [
    "## 2.1.3.Bonus: Other wavelet mother wavelet functions\n",
    "Can you experiment with other mother wavelet functions? Search the literature for particularly effective ones for PCG recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEcnhdV9dq1p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMpLNlf4IbuW"
   },
   "source": [
    "## 2.1.3.a - Universal Thresholding\n",
    "Since the DWT provides a multiresolution decomposition of the signal, we can use our **prior knowledge** that *most of the information is concentrated in the low frequencies* to mitigate noise in an adaptative fashion. Rembember, we may not want to get rid of all high frequency content necesseraly!\n",
    "\n",
    "\n",
    "\n",
    "Suppose that the detail coefficients at the **finest scale** are distributed according to a standard Gaussian scaled by $\\sigma$. Then, using [extreme value theory](https://nobel.web.unc.edu/wp-content/uploads/sites/13591/2019/11/Gaussian_Extremes-1.pdf), the largest detail coefficient is:\n",
    "\n",
    "$$\\max_{i=1, \\ldots, n} |D_i| \\approx O(\\sigma\\sqrt{2\\log n})$$\n",
    "\n",
    "The higher the frequency, the more sensitive to small perturbations our estimates will be, hence we are looking for a robust estimator of $\\sigma$. Typically one uses the **median absolute deviation** estimate of the Gaussian:\n",
    "\n",
    "$$\\sigma \\approx \\frac{\\text{median}(|D|)}{0.6745}$$\n",
    "\n",
    "\n",
    "\n",
    "Now, we only need to define the thresholding function. We will implement the following soft-thresholding:\n",
    "\n",
    "$$\\hat{D}_j = \\text{sign}(D_j) \\cdot \\max(|D_j| - \\lambda, 0)$$\n",
    "\n",
    "where $\\lambda = \\sigma\\sqrt{2\\log n}$. This effectively zeroes out coefficients smaller or equal to $\\lambda$ and shifts the remaining $D_j$s towards 0 by $\\lambda$.\n",
    "\n",
    "\n",
    "Implement a function that receives the recordings and outputs the a filtered version of the signal. Use `PyWavelets` package to do so, using `pywt.wavedec`to decompose the signal and `pywt.waverec` to reconstruct the signal (after applying universal thresholding to the coefficients).\n",
    "\n",
    "Run the cell below to see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJC43NRxxxN-"
   },
   "outputs": [],
   "source": [
    "def soft_threshold(coeff, threshold):\n",
    "        return np.sign(coeff) * np.maximum(np.abs(coeff) - threshold, 0)\n",
    "idx = 1\n",
    "max_duration = 1000\n",
    "n_levels = 5\n",
    "coeffs = pywt.wavedec(filtered_sounds[0][:max_duration], wavelet='haar', level=n_levels)\n",
    "# Plot the original signal alongside the wavelet coefficients\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Define a threshold value (e.g., universal threshold)\n",
    "sigma = np.median(np.abs(coeffs[-1])) / 0.6745  # Estimating noise level\n",
    "threshold = sigma * np.sqrt(2 * np.log(len(filtered_sounds[0][:max_duration])))\n",
    "\n",
    "# Apply soft thresholding to the detail coefficients\n",
    "coeffs_thresholded = [coeffs[0]]  # Keep approximation coefficients unchanged\n",
    "for coeff in coeffs[1:]:\n",
    "    coeffs_thresholded.append(soft_threshold(coeff, threshold))\n",
    "\n",
    "\n",
    "# Plot the original noisy signal\n",
    "plt.subplot(n_levels + 2, 1, 1)\n",
    "plt.plot(filtered_sounds[0][:max_duration], color='green')\n",
    "plt.title('Original Noisy Signal')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the approximation coefficients at the highest level\n",
    "plt.subplot(n_levels + 2, 1, 2)\n",
    "plt.plot(coeffs[0], color='blue')\n",
    "plt.title(f'Haar Approximation Coefficients (Level {n_levels})')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the detail coefficients for each level\n",
    "for i in range(1, n_levels + 1):\n",
    "    plt.subplot(n_levels + 2, 1, i + 2)\n",
    "    plt.plot(coeffs[i], color='red', label='original')\n",
    "    plt.plot(coeffs_thresholded[i], color='purple', label='after universal threshold')\n",
    "    plt.title(f'Haar Detail Coefficients (Level {n_levels - i + 1})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Mo-zATuXLuL"
   },
   "source": [
    "## 2.1.3.b. Bonus: design your own $\\sigma$ threshold\n",
    "\n",
    "We computed $\\sigma$ as a function of the detail coefficients at the finer scale. This could be improved upon by computing $\\sigma$ at each detail level. There are also more sophisticated thresholding strategies in the literature. Can you implement any of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lmzOB9qeA66"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efl4omzyzd6e"
   },
   "source": [
    "## 2.1.3.c Denoise all recordings\n",
    "Apply your wavelet denoising strategy for all recordings in the dataset.\n",
    "\n",
    "NOTE: if you designed your own custom filtering strategy you have to adapt `wavelet_denoising` method to support it. We recommend following you follow a similar recipe to `soft_threshold`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLamVLHDUdPl"
   },
   "outputs": [],
   "source": [
    "# TODO: dizer para incluirem o wavelet denoise que\n",
    "def soft_threshold(coeff, threshold):\n",
    "        return np.sign(coeff) * np.maximum(np.abs(coeff) - threshold, 0)\n",
    "\n",
    "def wavelet_denoising(sounds, wavelet_family='haar', n_levels=5):\n",
    "    # Perform the wavelet decomposition\n",
    "    denoised_sounds = []\n",
    "    for noisy_signal in tqdm(sounds, 'Wavelet Denoising', total=len(sounds), leave=True):\n",
    "        coeffs = pywt.wavedec(noisy_signal, wavelet=wavelet_family, level=n_levels)\n",
    "\n",
    "        # Define a threshold value (e.g., universal threshold)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745  # Estimating noise level\n",
    "        threshold = sigma * np.sqrt(2 * np.log(len(noisy_signal)))\n",
    "\n",
    "        # Apply soft thresholding to the detail coefficients\n",
    "        coeffs_thresholded = [coeffs[0]]  # Keep approximation coefficients unchanged\n",
    "        for coeff in coeffs[1:]:\n",
    "            coeffs_thresholded.append(soft_threshold(coeff, threshold))\n",
    "        denoised_sounds.append(pywt.waverec(coeffs_thresholded, wavelet_family))\n",
    "\n",
    "    return np.array(denoised_sounds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtbmwMg8UdPl"
   },
   "outputs": [],
   "source": [
    "wavelet_family= '' # select a wavelet family\n",
    "denoised_sounds = wavelet_denoising(dataset[:, 1], wavelet_family=wavelet_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose a sample and visualize the original sound and the sound after wavelet denoising."
   ],
   "metadata": {
    "id": "qnUliDuoRVxQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5IhHdUL4iFq"
   },
   "outputs": [],
   "source": [
    "def plot_denoised_signal(dataset, denoised_sounds, sample_idx):\n",
    "    noisy_signal = dataset[sample_idx, 1]\n",
    "    denoised_signal = denoised_sounds[sample_idx]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(noisy_signal, label='Noisy Signal')\n",
    "    plt.plot(denoised_signal, label='Denoised Signal', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.title(f'Signal denoising using {wavelet_family} wavelet')\n",
    "    plt.show()\n",
    "\n",
    "# Inspect the results on a sample\n",
    "sample_idx\n",
    "plot_denoised_signal(dataset, denoised_sounds, sample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Asssign the denoised sounds to the appropriate column.\n",
    "\n",
    "\n",
    "**Remember that this alters your dataset, so run it only when you are 100% once again!**"
   ],
   "metadata": {
    "id": "HIEAYIVIRtK6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fL5enz5UdPl"
   },
   "outputs": [],
   "source": [
    "dataset[:, 1] = denoised_sounds # assign denoised sounds to the recordings in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WOmmocj0UdPl"
   },
   "source": [
    "# 2.2 Homomorphic Envelope\n",
    "\n",
    "In this part you will be computing the Hilbert transform and the homomorphic envelope of the denoised heart sound signals. These representations allow separating the spectral components of a signal and thus can be particularly useful in discriminating between low-frequency events (such as the S1 and S2 sounds) from high-frequency noise or artifacts. By applying homomorphic filtering, the heart sound signal is decomposed into its envelope and excitation components.\n",
    "\n",
    "## 2.2.1.a Hilbert Transform\n",
    "\n",
    "The inner product between $\\sin$ and $\\cos$ over $[0, 2\\pi]$ is:\n",
    "\n",
    "$$\\int_0^{2\\pi} \\sin(\\omega t)\\cos(\\omega t) dt\n",
    "=0$$\n",
    "\n",
    "These functions are orthogonal, so they encode different information.\n",
    "\n",
    "Note that $sin(\\omega t) = \\cos(\\omega t + \\frac{\\pi}{2})$. So this phase shift operation permits one to inspect how the signal varies along a direction that is orthogonal to its own.\n",
    "\n",
    "The Hilbert transform is given by:\n",
    "\n",
    "$$x(t)^*= \\frac{1}{\\pi} \\int x(\\tau)\\cdot \\frac{1}{t-\\tau}d\\tau$$\n",
    "\n",
    "which is equivalent to $x * h(t)$, where $h(t)=\\frac{1}{\\pi t}$ is the *Hilbert kernel*. The above function can be seen as an inner product in the frequency domain due to the Convolution theorem; the Fourier transform of $h(t)$ is $-j\\, \\text{sgn}\\omega$, so this kernel effectively shifts the phase by $\\pm$90º of each frequency component of $x$.\n",
    "\n",
    "The *analytic* signal is thus given by\n",
    "\n",
    "$$ \\hat{x}(t)=x(t) + jx^*(t)$$\n",
    "where $x$ and $x^*$ can be seen as two basis functions, and $\\hat{x}$ the linear combination $[1, j]^\\top$ of them.\n",
    "\n",
    "The Hilbert envelope is given by\n",
    "\n",
    "$$H(t)= |\\hat{x}(t)| := \\sqrt{x(t)^2 + x^* (t)^2 }$$\n",
    "i.e., the Euclidean norm in the Complex plane. This is also called the *magnitude* of the analytic signal.\n",
    "\n",
    "\n",
    "Use `scipy.signal.hilbert` to compute the analytic signal.\n",
    "Extract the Hilbert envelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Hhae6s_UdPm"
   },
   "outputs": [],
   "source": [
    "sampling_frequency = 4000\n",
    "# Input signal (replace with your actual signal)\n",
    "sample_idx = 5\n",
    "input_signal = np.array(dataset[sample_idx, 1])\n",
    "\n",
    "analytic_signal = scipy.signal.hilbert(input_signal)  # Compute the analytic signal\n",
    "hilbert_envelope = np.abs(analytic_signal)  # Get the envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ust9E1GZt8T5"
   },
   "source": [
    "## 2.2.1.b Visualize the Hilbert Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV1emOZTUdPm"
   },
   "outputs": [],
   "source": [
    "def view_hilbert_transform(input_signal, analytic_signal, amplitude_envelope, n_samples=500):\n",
    "    # Plotting the original signal, the analytic signal, and the envelope\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the original signal\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(input_signal[:n_samples], label='Original Signal')\n",
    "    plt.title('Original Signal')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # Plot the real and imaginary parts of the analytic signal\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(analytic_signal.real[:n_samples], label='Real part of Analytic Signal')\n",
    "    plt.plot(analytic_signal.imag[:n_samples], label='Imaginary part of Analytic Signal', linestyle='--')\n",
    "    plt.title('Analytic Signal (Real and Imaginary Parts)')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the amplitude envelope\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(input_signal[:n_samples], label='Original Signal')\n",
    "    plt.plot(hilbert_envelope[:n_samples], label='Hilbert Envelope', linewidth=2)\n",
    "    plt.title('Hilbert Envelope')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N-mpiceUdPm"
   },
   "outputs": [],
   "source": [
    "view_hilbert_transform(input_signal, analytic_signal, hilbert_envelope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2oE684b8UdPm"
   },
   "source": [
    "## 2.4.3 Homomorphic Filtering\n",
    "\n",
    "The analytic signal can be described as\n",
    "\n",
    "$$\\hat{x}(t)=A(t)\\cdot c(t),$$\n",
    "i.e., a slowly variying amplitude function $A$ and a **carrier** high frequency signal $c(t)$.\n",
    "\n",
    "The $\\log$-transform makes this relationship additive, so one can effectively use low-pass filters (LPFs) to remove or mitigate high-frequency noise in the carriers.\n",
    "\n",
    "$$  A_{\\text{homomorphic}}(t) = \\exp\\left(\\text{LPF}\\{\\log H(t)\\}\\right) $$\n",
    "\n",
    "Use an order 2 low-pass Butterworth filter at 640 Hz to compute the Homomorphic envelop. Use `scipy.signal.butter` for this effect. This is parametrization follows from the work of [Springer et al.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7234876)\n",
    "\n",
    "Remember that the sampling rate is 4000Hz. Visaulize the envelope using `view_envelograms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7atQUcS4UdPm"
   },
   "outputs": [],
   "source": [
    "sampling_frequency = 4000\n",
    "# Define filter parameters\n",
    "lpf_frequency = 640   # Since we are using 4KHz (8 * (4000/50))\n",
    "\n",
    "\n",
    "B_low, A_low = scipy.signal.butter(N=1, Wn=2 * lpf_frequency / sampling_frequency, btype='low')\n",
    "# Apply the low-pass filter on the log of the hilbert envelope\n",
    "homomorphic_envelope = np.exp(scipy.signal.filtfilt(B_low,\n",
    "                                                    A_low,\n",
    "                                                    np.log(hilbert_envelope)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tYakoP3UdPm"
   },
   "outputs": [],
   "source": [
    "def view_envelograms(input_signal,\n",
    "                           analytic_signal,\n",
    "                           hilbert_envelope,\n",
    "                           homomorphic_envelope,\n",
    "                           n_samples=500):\n",
    "    # Plotting the original signal, the analytic signal, and the envelope\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the original signal\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(input_signal[:n_samples], label='Original Signal')\n",
    "    plt.title('Original Signal')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # Plot the real and imaginary parts of the analytic signal\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(analytic_signal.real[:n_samples], label='Real part of Analytic Signal')\n",
    "    plt.plot(analytic_signal.imag[:n_samples], label='Imaginary part of Analytic Signal', linestyle='--')\n",
    "    plt.title('Analytic Signal (Real and Imaginary Parts)')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the amplitude envelope\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(input_signal[:n_samples], label='Original Signal')\n",
    "    plt.plot(hilbert_envelope[:n_samples], label='Hilbert Envelope', linewidth=2)\n",
    "    plt.title('Hilbert Envelope')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(input_signal[:n_samples], label='Original Signal')\n",
    "    plt.plot(homomorphic_envelope[:n_samples], label='Amplitude Envelope', linewidth=2)\n",
    "    plt.title('Homomorphic Envelope (Springer et al.)')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6AXT7UXUdPm"
   },
   "outputs": [],
   "source": [
    "view_envelograms(input_signal, analytic_signal, hilbert_envelope, homomorphic_envelope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhCb6V30Xx7K"
   },
   "source": [
    "## 2.4.3.Bonus\n",
    "Search the literature for other envelograms used for PCG segmentation. Can you implement them? What different features do they capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzueeGvOfPB4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9jJtHizjUdPm"
   },
   "source": [
    "## 2.4.3 Compute the Normalized envelograms\n",
    "Normalize each envelope for each recording so it has mean 0 and unit variance.\n",
    "\n",
    "Note that you need to adapt the code hereinafter should you wish to include your custom envelograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrF4OkH3UdPm"
   },
   "outputs": [],
   "source": [
    "normalize_signal = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "normalized_hilbert = normalize_signal(hilbert_envelope)\n",
    "normalized_homomorphic = normalize_signal(homomorphic_envelope)\n",
    "\n",
    "plt.plot(normalized_hilbert)\n",
    "plt.plot(normalized_homomorphic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5xiishG4UdPm"
   },
   "source": [
    "# 2.4.3 Implement the *normalized* envelops for all dataset.\n",
    "The new dataset should have the following columns in order:\n",
    "\n",
    "*   recording name\n",
    "*   filtered PCG\n",
    "*   normalized Hilbert envelogram\n",
    "*   normalized Homomorphic envelogram\n",
    "*   any additional bonus normalized envelograms\n",
    "*   heart sound sequential labels\n",
    "*   murmur categorical labels\n",
    "\n",
    "In this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vY68rs8UdPq"
   },
   "outputs": [],
   "source": [
    "normalize_signal = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "def extract_evelograms(dataset, sampling_rate=4000, low_pass_fs=640):\n",
    "    dataset_out = np.zeros([dataset.shape[0], 6], dtype=object)\n",
    "    dataset_out[:, :4] = dataset\n",
    "\n",
    "    B_low, A_low = scipy.signal.butter(N=1, Wn=2 * low_pass_fs / sampling_rate, btype='low')\n",
    "    for i, recording in tqdm(enumerate(dataset[:, 1]), 'Extracting Envelograms', total=len(dataset), leave=True):\n",
    "        analytic_signal = scipy.signal.hilbert(recording)  # Compute the analytic signal\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        homomorphic_envelope = np.exp(scipy.signal.filtfilt(B_low, A_low, np.log(amplitude_envelope)))\n",
    "        # any custom envelograms here\n",
    "\n",
    "        # last two columns for labels!\n",
    "        dataset_out[i, -2] = normalize_signal(amplitude_envelope)\n",
    "        dataset_out[i, -1] = normalize_signal(homomorphic_envelope)\n",
    "    return dataset_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPgT44lqUdPr"
   },
   "outputs": [],
   "source": [
    "# This code has to be adapted if you have more than 2 envelograms!!!\n",
    "dataset_out = extract_evelograms(dataset)\n",
    "new_order = dataset_out[:, [1, 4, 5, 2, 3]]\n",
    "\n",
    "\n",
    "dataset_out[:, 1:] = new_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IXyl4I4HUdPr"
   },
   "source": [
    "## 2.5. Downsample\n",
    "Resample the all signals from 4000 Hz to 50 Hz, i.e.,  the envelograms and heart sound sequential labels of the dataset.\n",
    "\n",
    "Downsampling to 50 Hz is of  fundamental importance for the following segmentation steps (to be implemented in the second part of the tutorial), in order to reduce the computational complexity of the segmentator.\n",
    "\n",
    "Note that the labels are discrete.\n",
    "\n",
    "If you have any additional envelograms, do not forget to adapt this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKTjlH08UdPr"
   },
   "outputs": [],
   "source": [
    "# TODO: ja dado\n",
    "def resample_signal(dataset,\n",
    "                    original_rate=4000,\n",
    "                    new_rate=50):\n",
    "    for i in tqdm(range(len(dataset)), # columns with features\n",
    "                                          'Resampling recordings', total=len(dataset), leave=True):\n",
    "        sound = dataset[i, 1]\n",
    "        hilb = dataset[i, 2]\n",
    "        homo = dataset[i, 3]\n",
    "        label = dataset[i, 4]\n",
    "        time_secs = len(sound) / original_rate\n",
    "        number_of_samples = int(time_secs * new_rate)\n",
    "\n",
    "        # Resample sound (continuous)\n",
    "        dataset[i, 1] = scipy.signal.resample(sound, number_of_samples).squeeze()\n",
    "        dataset[i, 2] = scipy.signal.resample(hilb, number_of_samples).squeeze()\n",
    "        dataset[i, 3] = scipy.signal.resample(homo, number_of_samples).squeeze()\n",
    "\n",
    "        # Ensure labels are in a numeric format (int or float)\n",
    "        label = np.array(label, dtype=np.int32)  # Convert labels to integers\n",
    "        # Create new time indices for the resampled labels\n",
    "        original_indices = np.arange(len(label))\n",
    "        new_indices = np.linspace(0, len(label) - 1, number_of_samples)\n",
    "\n",
    "        # Resample labels by nearest-neighbor interpolation\n",
    "        resampled_label = np.round(np.interp(new_indices, original_indices, label)).astype(int)\n",
    "        resampled_label = np.clip(resampled_label, 1, 4)  # Ensure values are within [1, 2, 3, 4]\n",
    "        dataset[i, 4] = resampled_label.squeeze()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d17sde-AUdPr",
    "outputId": "4352c7c4-152a-45b2-f13d-f61a08967799"
   },
   "outputs": [],
   "source": [
    "dataset_out_resampled = resample_signal(dataset_out.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZwcOS63UdPr"
   },
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "sample = dataset_out_resampled[sample_idx]\n",
    "plot_sound_and_label(x=sample[1], y=sample[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "URdBJ-6WUdPr"
   },
   "source": [
    "#  2.6. Splits per patient\n",
    "We will now generate the train, validation, and test splits for the second part of the tutorial. If you have custom features that you wish to include pay extra attention and adapt the code when needed.\n",
    "\n",
    "We will fetch patient recordings by parsing the wavfile name. We will produce three sets `train`, `val`, and `test` so that they are mutually exclusive in terms of patient ids.\n",
    "\n",
    "Remember from 2. that each wavfile has the patient_id and ausculation location divided seperated by characterd `_`. We will use this to fetch all unique patient identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G49HqpqrUdPr"
   },
   "outputs": [],
   "source": [
    "def patient_ids_only(patients_col):\n",
    "        def pattern_match(pattern, string):\n",
    "            start, end = re.search(pattern, string).span()\n",
    "            return string[start:end]\n",
    "\n",
    "        return np.array([int(pattern_match('\\d*', patient)) for patient in patients_col])\n",
    "\n",
    "patient_ids = patient_ids_only(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will add the patient_ids as a column to the dataset."
   ],
   "metadata": {
    "id": "og4ClA11UOas"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLCg7Et2UdPr",
    "outputId": "1ce6e300-d7d6-4c53-801a-598857fd3be5"
   },
   "outputs": [],
   "source": [
    "dataset_out_final = np.zeros([dataset_out_resampled.shape[0], 7], dtype=object)\n",
    "dataset_out_final[:, 1:] = dataset_out_resampled\n",
    "dataset_out_final[:, 0] = patient_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sR5h4XgBUdPr"
   },
   "source": [
    "# Use the below function to create train val and test splits\n",
    "Use a test_split size of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_grUxxicUdPr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def extract_and_save_splits(data, split_size=.2):\n",
    "    # Extract patient IDs (assuming they are in row 0)\n",
    "    patient_ids = data[:, 0].astype(np.int32)\n",
    "\n",
    "    # Get the unique patient IDs\n",
    "    unique_patient_ids = np.unique(patient_ids)\n",
    "    # Step 1: Split unique patient IDs into train and remaining (val + test)\n",
    "    train_ids, val_test_ids = train_test_split(unique_patient_ids, test_size=split_size, random_state=42)\n",
    "\n",
    "    # Step 2: Split val_test_ids into validation and test sets\n",
    "    val_ids, test_ids = train_test_split(val_test_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Step 3: Create masks for train, validation, and test based on patient IDs\n",
    "    train_mask = np.isin(patient_ids, train_ids)\n",
    "    val_mask = np.isin(patient_ids, val_ids)\n",
    "    test_mask = np.isin(patient_ids, test_ids)\n",
    "\n",
    "    # Step 4: Apply the masks to create train, val, and test sets\n",
    "    train_data = data[train_mask]\n",
    "    val_data = data[val_mask]\n",
    "    test_data = data[test_mask]\n",
    "    print(f'Train size: {len(train_data)}; Val size: {len(val_data)}; Test size: {len(test_data)}')\n",
    "    np.save('my_train_data.npy', train_data)\n",
    "    np.save('my_val_data.npy', val_data)\n",
    "    np.save('my_test_data.npy', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rz21rZHUdPr",
    "outputId": "c5bc645e-6958-4535-a676-9d70a6a2e8a7"
   },
   "outputs": [],
   "source": [
    "extract_and_save_splits(dataset_out_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb_xMVN9onZc"
   },
   "source": [
    "# Final submission\n",
    "Send your .ipynb files to francesco.renna@fc.up.pt as well as 3 slides summarizing your approaches for both tutorials 1 and 2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
